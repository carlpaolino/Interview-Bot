[
{"file": "dsa_slides/01_IntroBigOMergeSortMaster.pdf", 
"content": "CS 3510 Algorithms: This Class + Big-O Notation Aaron Hillegass Georgia Tech\n2/47 I Was In Over My Head\n3/47 What Came Next • Worked on Wall Street • Worked with Steve Jobs • Part of the NeXT/Apple merger • Founded Big Nerd Ranch • Published four books • Sold Big Nerd Ranch • Went back to school at Georgia Tech\n4/47 This Class Study theoretical properties of algorithms: • Correctness • Computational complexity • Memory requirements Psuedocode. Mathematical Proofs. No actual coding. 4 exams, 18% each = 72% Homeworks = 20% In-class worksheets: 8%\n5/47 Office Hours Me: Room 3 in Institut Lafayette. My hours will be Tuesday 1:30-3pm and Wednesday 9-11am. TA: Ameen Agbaria, Thursday from 3:00-4:30. Location TBD\n6/47 Why? • The best need to know if their solutions will scale. • This class will get you through a technical interview. • The algorithms are…fun!\n7/47 Parts • Divide and Conquer • Graph Algorithms • Dynamic Programming • NP-Completeness\n8/47 Correctness Always the first: Does the algorithm give correct answers? • Sound: If it returns an answer, the answer is correct. • Complete: • If a correct answer exists, the algorithm finds one in finite time. • If a correct answer doesn’t exist the algorithm reports this in finite time. Then we talk about speed/space.\nWhat we want to know: How long will this algorithm take?\n10/47 Linear scan of an array bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; } • From RAM to cache: 12 clock cycles (brings 64 bytes) • Cache to register: 1 clock cycle • Comparing two registers: 1 clock cycle\n11/47 Linear scan of array • Best case? • Average case? • Worst case?\n12/47 Problems with counting clock cycles • Hardware-dependent • Hassle • The differences we are looking for are BIG\nWhat we are satified knowing: How much longer will this algorithm take if we double the size of the data?\n14/47 Find in an unsorted array If 𝑛 doubles, the time doubles. 𝑂(𝑛) means “In the worst case, time scales linearly with 𝑛” bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; }\n15/47 Intuition 𝑂(1): Double 𝑛? Time stays the same. 𝑂(log 𝑛): Double 𝑛? Time increases by a constant amount. 𝑂(𝑛): Double 𝑛? Time doubles. 𝑂(𝑛2): Double 𝑛? Time × 4 𝑂(𝑛3): Double 𝑛? Time × 8 𝑂(2𝑛): Increase 𝑛 by 1? Time doubles. 𝑂(𝑛𝑛): Increase 𝑛 by 1? Weep.\n16/47 Definition of Big-O 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛)) means: There exists a 𝐶 > 0 and an 𝑛 such that 0 𝑓(𝑛) ≤ 𝐶𝑔(𝑛) for all 𝑛 ≥ 𝑛 0\n17/47 Properties Multiplying by a constant doesn’t matter. In sum, only fastest growing (“dominating”) term matters. 1 Note: the base of the log doesn’t matter: log 𝑛 = ( )(log 𝑛) 𝑎 log 𝑎 𝑏 𝑏 Exponentials always dominate polynomials Polynomials always dominate logs 𝑂(1) < 𝑂(log 𝑛) < 𝑂(𝑛) < 𝑂(𝑛2) < 𝑂(2𝑛) < 𝑂(3𝑛) < 𝑂(𝑛!) < 𝑂(𝑛𝑛)\n𝑂 Θ Ω 18/47 Big vs Big vs Big 𝑓(𝑛) ∈ Ω(𝑔(𝑛)) 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛)) 𝑓(𝑛) ∈ Θ(𝑔(𝑛)) There ∃𝐶, 𝑛 such that There ∃𝐶, 𝑛 such that There ∃𝐶 , 𝐶 , 𝑛 such 0 0 1 2 0 that 𝑓(𝑛) ≥ 𝐶𝑔(𝑛) 𝑓(𝑛) ≤ 𝐶𝑔(𝑛) 𝐶 𝑔(𝑛) ≤ 𝑓(𝑛) ≤ 1 for all 𝑛 > 𝑛 for all 𝑛 > 𝑛 𝐶 𝑔(𝑛) 0 0 2 Or: 𝑔(𝑛) ∈ 𝑂(𝑓(𝑛)) for all 𝑛 > 𝑛 0 Or: 𝑔(𝑛) ∈ 𝑂(𝑓(𝑛)) and 𝑓(𝑛) ∈ 𝑂(𝑔(𝑛))\n19/47 Fibonacci Numbers 0,1,1,2,3,5,8,13,… Inductive definition: • 𝐹 = 𝐹 + 𝐹 𝑛 𝑛−1 𝑛−2 • 𝐹 = 0, 𝐹 = 1 0 1\n20/47 Fibonacci Numbers Recursively 1 define fib(n): 2 if 𝑛 == 0 or 𝑛 == 1: 3 return 𝑛 4 return fib(n-1) + fib(n-2) What can we say about 𝑇 (𝑛), the number of steps to compute 𝐹 ? 𝑛\n21/47 Fibonacci Numbers Recursively 𝑇 (𝑛) = 𝑇 (𝑛 − 1) + 𝑇 (𝑛 − 2) + 𝐶 1 𝑇 (0) = 𝑇 (1) = 𝐶 0 For big enough 𝑛: 𝑇(𝑛 − 1) > 𝑇(𝑛 − 2) + 𝐶 > 𝑇(𝑛 − 2) − 𝐶 1 1 2𝑇(𝑛 − 1) > 𝑇(𝑛) > 2𝑇(𝑛 − 2) 2𝑛 > 𝑇(𝑛) > 2 𝑛 2 √ 𝑛 2𝑛 > 𝑇 (𝑛) > ( 2)\n22/47 Fibonacci Numbers Recursively 𝑇(𝑛) = 𝑇(𝑛 − 1) + 𝑇(𝑛 − 2) + 𝐶 1 𝑇(0) = 𝑇(1) = 𝐶 0 For big enough 𝑛: √ 𝑛 2𝑛 > 𝑇(𝑛) > ( 2) fib(𝑛) ∈ 𝑂(2𝑛) √ 𝑛 fib(𝑛) ∈ Ω(( 2) )\n23/47 Fibonacci Numbers Recursively 𝑇(𝑛) = 𝑇(𝑛 − 1) + 𝑇(𝑛 − 2) + 𝐶 1 𝑇(𝑛) = 𝑎𝑛 𝑎𝑛 = 𝑎𝑛−1 + 𝑎𝑛−2 + 𝐶 1 𝐶 𝑎2 = 𝑎 + 1 + 1 𝑎𝑛−2 √ 1+ 5 𝑎 = = 𝜑 ≈ 1.618 2 fib(n) ∈ Θ(𝜑𝑛)\n24/47 Dumb Recursion\n25/47 Fibonacci Numbers: Dynamic Programming 1 define fib2(n): 𝑎 ≔ array[0.. = 𝑛] 2 3 𝑎[0] ≔ 0, 𝑎[1] ≔ 1 4 for 𝑖 from 2 to 𝑛: 𝑎[𝑖] ≔ 𝑎[𝑖 − 1] + 𝑎[𝑖 − 2] 5 fib2(𝑛) ∈ 𝑂(𝑛) 6 return 𝑎[𝑛]\n26/47 Fibonacci Numbers: Using Matrices If you know 𝐹 and 𝐹 you can compute 𝐹 using a matrix: 𝑘 𝑘+1 𝑘+2 𝐹 1 1 𝐹 ( 𝑘+2) = ( )( 𝑘+1) 𝐹 1 0 𝐹 𝑘+1 𝑘 Starting with 𝐹 = 0 and 𝐹 = 1, we can compute 𝐹 and 𝐹 for any 𝑛: 0 1 𝑛+1 𝑛 𝑛 𝐹 1 1 1 ( 𝑛+1) = ( ) ( ) 𝐹 1 0 0 𝑛 How does matrix exponentation scale?\n27/47 Exponentiation by Squaring You want to know 𝑎20 ? What is 20 in binary? 10100 So 𝑎20 = 𝑎16𝑎4 Exponentiation by 𝑛 is 𝑂(log 𝑛).\n28/47 Exponentiation Diagonalizable Matrices Matrix 𝐵 is diagonalizable if it can be written as 𝐵 = 𝑃 𝐷𝑃 {−1} where 𝐷 is a diagonal matrix. Assume 𝐵 is diagonalizable. 𝐵3 = (𝑃 𝐷𝑃 −1)(𝑃 𝐷𝑃 −1)(𝑃 𝐷𝑃 −1) = 𝑃 𝐷3𝑃 −1\n29/47 Fibonacci: Binet's Formula 1 1 ( ) is diagonalizable: 1 0 𝜑 −𝜑−1 𝑃 = ( ) 1 1 𝜑 0 𝐷 = ( ) 0 −𝜑−1 𝑃 −1 = ( 1 )( 1 𝜑−1 ) √ 5 −1 𝜑 √ 1+ 5 (Reminder: 𝜑 = ≈ 1.6180339887) 2 𝑛 𝐹 1 1 1 1 ( 𝑛+1) = ( ) ( ) = 𝑃 𝐷𝑛𝑃 −1( ) 𝐹 1 0 0 0 𝑛 √ 𝑛 √ 𝑛 1 1+ 5 1− 5 𝐹 = ( )(( ) − ( ) ) √ 𝑛 5 2 2\n30/47 Comparison • Recursive: 𝑂(𝜑𝑛) • Dynamic Programming: 𝑂(𝑛) • Matrix Exponentiation: 𝑂(log 𝑛) • Binet’s Formula: 𝑂(1)\n31/47 Questions we won’t ask in this class bool contains(uint16_t *buf, usize n, uint16_t x) { for (usize i = 0; i < n; i++) { if buf[i] == x { return true; } } return false; } • Can it be parallelized? Vectorized? • Locality of reference? Cache hits? • Could we speed it up via prefetching? • How hard would it be to make the data persistent?\nWhy are computer science professors so obsessed with sorting!?\n33/47 Comparison Sorting Algorithms • Serial? 𝑂(𝑛 log 𝑛) • Stable: equal values stay in the same order • 𝑂(1) memory usage for in place, 𝑂(𝑛) otherwise • Most good algorithms are 𝑂(𝑛) for best case\n34/47 1 define Merge(𝑔, ℎ) -> sorted list: 𝑐 ≔ new List 2 3 while 𝑔 and ℎ are non-empty: 4 if head(𝑔) ≤ head(ℎ): 𝑐.append(𝑔.pop_head()) 5 6 else 𝑐.append(ℎ.pop_head()) 7 8 if 𝑔 is not empty: 9 Append everying in 𝑔 to 𝑐 10 if ℎ is not empty: 11 Append everything in ℎ to 𝑐 12 return 𝑐 𝑂(𝑛) where 𝑛 is len(𝑔) + len(ℎ)\n35/47 MergeSort 1 define MergeSort(𝑔) → sorted list: 𝑛 ≔ length of 𝑔 2 3 if 𝑛 == 0 or 𝑛 == 1, return g 𝑛 𝑠 = ⌊ ⌋ 4 2 left ≔ MergeSort(prefix of 𝑔 of length 𝑠) 5 right ≔ MergeSort(rest of 𝑔) 6 7 return Merge(left, right) Time complexity?\n36/47 MergeSort Time Complexity • How many layers of merges? log (𝑛) 2 • Complexity of each layer? 𝑂(𝑛) Total time complexity: 𝑂(𝑛 log 𝑛)\nCan we come up with a general rule for the time complexity of divide-and- conquer algorithms?\n38/47 MergeSort Revisited 1 define MergeSort(𝑔) -> sorted list: 𝑛 ≔ length of 𝑔 2 • MergeSort divides into 3 if 𝑛 == 0 or 𝑛 == 1: two MergeSorts on half 𝑛 4 return g • The cost of this is the 𝑂(𝑛) 𝑠 = ⌊𝑛⌋ 5 2 merge. left ≔ MergeSort(𝑔[1..𝑠]) 6 right ≔ MergeSort(𝑔[𝑠 + 1..𝑛]) 7 𝑇 (𝑛) = 2𝑇 ( 𝑛 ) + 𝑂(𝑛1) 2 8 return Merge(left,right) Generally: 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏\n39/47 Master Theorem Stated Given: 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 Then: 𝑂(𝑛𝑑) if 𝑑>log 𝑎 𝑏 𝑇 (𝑛) = {𝑂(𝑛𝑑 log𝑛) if 𝑑=log 𝑎 𝑏 𝑂(𝑛log 𝑏 𝑎) if 𝑑<log 𝑎 𝑏\n40/47 Applied to MergeSort 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 For MergeSort: 𝑎 = 2, 𝑏 = 2, 𝑑 = 1 Case: 𝑑 = log 𝑎 𝑏 MergeSort has 𝑂(𝑛1 log 𝑛) or just 𝑂(𝑛 log 𝑛)\n41/47 Proof: Consider 𝑇 (𝑛) = 𝑎𝑇 ( 𝑛 ) + 𝑂(𝑛𝑑) 𝑏 𝑇 (𝑛) = ∑ log 𝑏 𝑛 𝑎𝑖( 𝑛 ) 𝑑 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑖\n𝑎 log 𝑛 = 𝑛 log 𝑎 42/47 Why 𝑏 𝑏 Remember change of base formula: log 𝑦 log 𝑦 = 𝑐 𝑥 log 𝑥 𝑐 Thus log 𝑎 1 log 𝑎 = 𝑎 = 𝑏 log 𝑏 log 𝑏 𝑎 𝑎 Thus log𝑎𝑛 log 𝑎 𝑎log 𝑛 = 𝑎 = 𝑎(log 𝑛)(log 𝑎) = (𝑎log 𝑛) 𝑏 = 𝑛log 𝑎 𝑏 log𝑎𝑏 𝑎 𝑏 𝑎 𝑏\n43/47 Simplify 𝑇 (𝑛) = ∑ log 𝑏 𝑛 𝑎𝑖( 𝑛 ) 𝑑 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑖 𝑇 (𝑛) = 𝑛𝑑 ∑ log 𝑏 𝑛 ( 𝑎 ) 𝑖 + 𝑂(𝑛log 𝑎) 𝑏 𝑖=0 𝑏𝑑 • 𝑑 > log 𝑎, 𝑎 < 1, first term of geometric series dominates: 𝑂(𝑛𝑑) 𝑏 𝑏𝑑 • 𝑑 < log 𝑎 : leaf term dominates: 𝑂(𝑛log 𝑏 𝑎) 𝑏 • 𝑑 = log 𝑎 : 𝑎 = 1, every term has the same value: 𝑂(𝑛𝑑 log 𝑛) 𝑏 𝑏𝑑\nMergeSort: What if instead of dividing 𝑘 the list in half, we divide it into parts?\n45/47 Apply It Apply Master Theorem to Binary Sort 𝑂(𝑛𝑑) if 𝑑>log 𝑎 𝑏 𝑇 (𝑛) = {𝑂(𝑛𝑑 log𝑛) if 𝑑=log 𝑎 𝑏 𝑂(𝑛log 𝑏 𝑎) if 𝑑<log 𝑎 𝑏\n46/47 Next Read chapter 0 and chapter 1 Next lecture: FFT and Arithmetic\nQuestions? Slides by Aaron Hillegass"},
{"file": "dsa_slides/01a_Notes.pdf", 
"content": "CS 3510 Algorithms 1/09/2023 Lecture 1: Introduction, Fibonacci, and Big-O Lecturer: Abrahim Ladha Scribe(s): Saigautam Bonam Why are algorithms so important? Primarily it is a useful, interesting, and foundational theory. It can also help you make a lot of money. Analgorithmisaprocesstocomputesomething. Theruntimeofanalgorithmisthenumber of steps it takes as a function of the input size. 1 Fibonacci Consider the problem of finding the nth Fibonacci number. These are 0,1,1,2,3,5,8,... and so on, beginning from F . There is a well-known recurrence with a base case of F = 0 0 0 and F = 1, and F = F +F . This recurrence gives us an obvious algorithm: 1 n n−1 n−2 def f1(n): if n == 0, 1: return n else: return f1(n - 1) + f1(n - 2) How many steps does this algorithm take? Let T(n) be the number of steps the algorithm takes on input n. Note that the time is dependent on its recursive calls, so T(n) = T(n−1)+T(n−2)+3 Note that 3 is just a constant that describes the amount of extra work outside of the recur- sive calls. We’ll see soon that the actual number doesn’t matter. We have that T(n−1) > T(n−2), then T(n) > 2T(n−2)+3. If we use the same idea, we will get T(n) > 4T(n−4)+3 and so on. This ultimately gets us to the following inequality: T(n) > 2n/2 This means T(n) grows exponentially with respect to n This is very bad and slow!. Practi- cally, I ran this for 43 days to compute F on a server and someone turned off the server 70 before it finished. Slow algorithms don’t simply mean you wait longer for the answer, but you may experience the heat death of the universe before you get an answer. You may also notice by the recurrence we can see that T(n) > F , but the Fibonacci’s also grow n exponentially. 1: Introduction, Fibonacci, and Big-O-1\nF n F F n−1 n−2 F F F F n−2 n−3 n−3 n−4 ... ... ... Note in the recursion tree, we actually recompute a massive amount of what is needed. As a human, this is not the way we compute Fibonacci numbers! We do so iteratively, writing down and re-using previous answers. Our pen-and-paper process of computing the Fibonacci’s is motivation for our next algorithm: def f2(n): if n == 0: return 0 arr = [0] * (n + 1) arr[0] = 0 arr[1] = 1 for i in range(2, n + 1): arr[i] = arr[i - 1] + arr[i - 2] return arr[n] We loop n times, and at each loop iteration we perform one addition. For now let’s overes- timate this addition takes n time (based on the number of bits of each number), and so we get that an upper bound for this algorithm is T(n) < n2. This shows the algorithm is faster than the previous one. Let’s examine another approach to Fibonacci. Consider the following: (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 1 = 1 1 1 1 (cid:20) (cid:21)2(cid:20) (cid:21) (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 0 1 1 1 = = 1 1 1 1 1 1 2 (cid:20) (cid:21)n(cid:20) (cid:21) (cid:20) (cid:21) 0 1 0 F = n 1 1 1 F n+1 Multiplication by by the matrix computes Fibonacci numbers! def f3(n): A = [[0 1][1 1]] v = [0 1] compute An compute Anv return F n 1: Introduction, Fibonacci, and Big-O-2\nT(n) = n−1 matrix multiplications, one vector multiplication and some constant work. T(n) = (n−1)MMs+(1)VM +c Let’s not worry about the costs of a 2×2 matrix multiplication yet, but the number, just for now. Can we reduce the number of matrix multiplications? By repeatedly squaring like A = A·A we only need to perform log(n) matrix multiplications. We will go into this algorithm for exponentiation in lecture three. T(n) = (logn)MMs+(1)VM +c You may recall diagonalization of a matrix. If a matrix A is diagonalizable, there exists D,P where A = PDP−1, where (cid:20) (cid:21) λ 0 D = 1 0 λ 2 We only care about this since An = (PDP−1)n = (PDP−1)(PDP−1)(PDP−1)... = PD(P−1P)D(P−1P)D... = PDnP−1 That’s helpful to us because (cid:20) λn 0 (cid:21) Dn = 1 0 λn 2 Although we don’t know it yet, computing PDnP−1 is much more efficient than computing An. def f4(n): D = ... P = ... P−1 = ... v = ... compute Dn F = PDnP−1v n return F n What is our runtime? we have two matrix multiplications, a vector multiplication, and two integer exponentiations to power n. T(n) = (2)MMs+(1)VM +(2)exps+c We don’t have the tools yet to know this is faster. But the moral here is that a deeper knowledge of mathematical tools can only improve run time. In fact, using this, we can derive a closed formula for F . Rather than plug in n, then compute the matrix product, n compute the matrix produce in terms of some variable n and then simplify it. Computing An = PDnP−1 algebraically, we get the closed form 1: Introduction, Fibonacci, and Big-O-3\n(cid:32) √ (cid:33)n (cid:32) √ (cid:33)n 1 1+ 5 1 1− 5 F = √ − √ n 5 2 5 2 Note that analyzing the runtime becomes more difficult. I want you to think about what it costs to compute powers of roots. 2 Big-O Let us discuss how to better analyze runtime. What is a “basic computer step”? Is it an instruction? We consider the runtime in the size of the input, denoted as n. It could be an array of size n or numbers with n bits. Computing the exact number of steps can be unnecessarily complicated. We use big O notation to get to the heart of the matter. For f,g : N → R, we say f(n) is O(g(n)) if there is a constant c such that f(n) ≤ c(g(n)) for large enough n. Note we care about asymptotics, and y may be less than f on small n. Most texts say f(n) = O(g(n)), but I like to say “is”. This big-O is a property of f rather than its equivalence. f(n) We say f(n) is o(g(n)) (little-oh) if lim = 0. Although this is a formal definition, n→∞ g(n) you may think of f(n) is o(g(n)) to mean that f grows strictly less than g asymptotically. f is never o(f). We say f(n) is Ω(g(n)) if there exists a constant c such that f(n) ≥ c(g(n)) for large enough n. We say that f(n) is Θ(g(n)) if f(n) = Ω(g(n)) and f(n) = O(g(n)). They are matching upper and lower bounds. f(n) We say that f(n) is ω(g(n)) if lim diverges. n→∞ g(n) Althoughyoushouldusetheformaldefinitionsofallofthese, itmaybehelpfultoremember them like this: O → f ≤ g Ω → f ≥ g o → f < g ω → f > g Θ → f = g Recall the following common bounds you might have seen already: O(1),O(logn),O(n),O(nlogn),O(n2),O(2n),O(n!) 1: Introduction, Fibonacci, and Big-O-4\nand more. Note that most algorithms will be at least Ω(n), as it usually takes at least n time to process the input (for example, an array of n integers). Any algorithm that doesn’t look at the whole input may be faster (for example, binary search). 1: Introduction, Fibonacci, and Big-O-5"}
]